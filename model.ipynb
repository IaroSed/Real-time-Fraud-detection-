{"cells":[{"cell_type":"code","source":["#/FileStore/tables/creditcard.csv\n#Loading the dataset\ndata = spark.read.csv(\"/FileStore/tables/creditcard.csv\",inferSchema=True, header=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Transforming seconds into hours - dividing by 3600 and taking modulo 24\nfrom pyspark.sql.types import StructType,DoubleType,IntegerType\nfrom pyspark.sql.functions import round\n\ndata = data.withColumn(\"Hour\", round(data.Time.cast(DoubleType())/3600,0) % 24)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n#import seaborn as sn\nimport pandas as pd\nimport numpy as np"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#Correlation matrix\n#sn.set(style=\"white\", font_scale=2)\n\n#Compute the correlation matrix\n#corr = data.select('Hour','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28', 'Amount' ).toPandas().corr()\n\n# Generate a mask for the upper triangle\n#mask = np.zeros_like(corr, dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\n#Set up the matlplotlib figure\n#f, ax = plt.subplots(figsize=(18,15))\n#f.suptitle(\"Correlation Matrix\", fontsize = 40)\n\n# Generate a custome diverging colormap\n#cmap = sn.diverging_palette(220,10, as_cmap= True)\n\n#Draw the heatmap with the mask and correct aspect ratio\n#sn.heatmap(corr, mask=mask, cmap=cmap, vmax=0.3, center=0, square=True, linewidths=0.5,cbar_kws={\"shrink\": 0.5})\n\n#display(f)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#Splitting data into train and test sets\ntrain_data, test_data = data.randomSplit([0.7,0.3])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["#Taking care of the imbalanced set\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n\ntrain_data_p = train_data.select('Hour','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28', 'Amount', 'Class').toPandas()\n\n#Undersampling to balance the dataset\n#fraud_indices = np.array(train_data_p[train_data_p.Class == 1].index)\n#number_records_fraud = len(fraud_indices)\n\n#normal_indices = train_data_p[train_data_p.Class == 0].index\n\n#random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace=False)\n#random_normal_indices = np.array(random_normal_indices)\n\n#under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n\n#under_sample_train_data = train_data_p.iloc[under_sample_indices,:]\n\n\ny_train = train_data_p.iloc[:, 30:31]\nX_train = train_data_p.iloc[:, 0:30]\n\n\n#Oversampling to balance dataset using SMOTE\n\nX_train_resample_smote, y_train_resample_smote = SMOTE(kind='svm').fit_sample(X_train,y_train)\n\nX_train_resample_smote = pd.DataFrame(data = X_train_resample_smote, columns={'Hour','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28', 'Amount'})\ny_train_resample_smote = pd.DataFrame(data = y_train_resample_smote, columns = {'Class'})\n\nover_sample_train_data_smote = pd.concat([X_train_resample_smote, y_train_resample_smote], axis=1)\n\n#Oversampling to balance dataset using ADASYN\n\n#X_train_resample_adasyn, y_train_resample_adasyn = ADASYN().fit_sample(X_train,y_train)\n\n#X_train_resample_adasyn = pd.DataFrame(data = X_train_resample_adasyn, columns={'V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28', 'Amount'})\n#y_train_resample_adasyn = pd.DataFrame(data = y_train_resample_adasyn, columns = {'Class'})\n\n#over_sample_train_data_adasyn = pd.concat([X_train_resample_adasyn, y_train_resample_adasyn], axis=1)\n\n#Defining schema of the event (transaction)\nschema = StructType() \\\n        .add(\"Hour\", DoubleType()) \\\n        .add(\"V1\", DoubleType()) \\\n        .add(\"V2\", DoubleType()) \\\n        .add(\"V3\", DoubleType()) \\\n        .add(\"V4\", DoubleType()) \\\n        .add(\"V5\", DoubleType()) \\\n        .add(\"V6\", DoubleType()) \\\n        .add(\"V7\", DoubleType()) \\\n        .add(\"V8\", DoubleType()) \\\n        .add(\"V9\", DoubleType()) \\\n        .add(\"V10\", DoubleType()) \\\n        .add(\"V11\", DoubleType()) \\\n        .add(\"V12\", DoubleType()) \\\n        .add(\"V13\", DoubleType()) \\\n        .add(\"V14\", DoubleType()) \\\n        .add(\"V15\", DoubleType()) \\\n        .add(\"V16\", DoubleType()) \\\n        .add(\"V17\", DoubleType()) \\\n        .add(\"V18\", DoubleType()) \\\n        .add(\"V19\", DoubleType()) \\\n        .add(\"V20\", DoubleType()) \\\n        .add(\"V21\", DoubleType()) \\\n        .add(\"V22\", DoubleType()) \\\n        .add(\"V23\", DoubleType()) \\\n        .add(\"V24\", DoubleType()) \\\n        .add(\"V25\", DoubleType()) \\\n        .add(\"V26\", DoubleType()) \\\n        .add(\"V27\", DoubleType()) \\\n        .add(\"V28\", DoubleType()) \\\n        .add(\"Amount\", DoubleType()) \\\n        .add(\"label\", IntegerType())\n\n#under_sample_train_data = spark.createDataFrame(under_sample_train_data,schema=schema)\n\nover_sample_train_data_smote = spark.createDataFrame(over_sample_train_data_smote,schema=schema)\n\n#over_sample_train_data_adasyn = spark.createDataFrame(over_sample_train_data_adasyn,schema=schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.5/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["#Creating the features vector\nfrom pyspark.ml.feature import VectorAssembler\n\ncolumns=['Hour','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28', 'Amount']\n\nassembler = VectorAssembler(inputCols=columns, outputCol='features')\n\ntrain_data = assembler.transform(over_sample_train_data_smote).select('features', 'label')\n\n#test_data = assembler.transform(test_data.withColumnRenamed('Class', 'label')).select('features', 'label')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["test_data = assembler.transform(test_data.withColumnRenamed('Class', 'label')).select('features', 'label')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["#from pyspark.ml import Pipeline\n#from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n#from pyspark.ml.classification import RandomForestClassifier\n\n#rfc =  RandomForestClassifier(labelCol = 'Class', featuresCol = 'features', maxDepth = 21, numTrees = 42)\n#rfc =  RandomForestClassifier(labelCol = 'label', featuresCol = 'features')\n\n#Pipeline\n#pipeline = Pipeline(stages = [assembler, rfc])\n\n#Grid\n#grid = ParamGridBuilder() \\\n#      .addGrid(rfc.maxDepth, [20,21]) \\\n#      .addGrid(rfc.numTrees, [39,40]) \\\n#      .build()\n\n#Evaluator - AUC by default\n#ev = BinaryClassificationEvaluator()\n\n#K-fold Cross validation\n#cv = CrossValidator(estimator = pipeline, \\\n#                   estimatorParamMaps = grid, \\\n#                   evaluator = ev, \\\n#                   numFolds = 10)\n\n#rfc_model = cv.fit(over_sample_train_data_smote)\n\n#rfc_model.bestModel.stages[-1].extractParamMap()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\nrfc =  RandomForestClassifier(labelCol = 'label', featuresCol = 'features', maxDepth = 21, numTrees = 40)\n\nrfc_model = rfc.fit(train_data)\n\nrfc_preds = rfc_model.transform(test_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Plotting the confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_true = test_data.select(\"label\")\ny_true = y_true.toPandas()\n\ny_pred = rfc_preds.select(\"prediction\")\ny_pred = y_pred.toPandas()\n\ncnf_matrix = confusion_matrix(y_true, y_pred)\ncnf_matrix"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">25</span><span class=\"ansired\">]: </span>\narray([[85316,    18],\n       [   28,   123]])\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["recall = cnf_matrix[1][1]/(cnf_matrix[1][0]+cnf_matrix[1][1])\nprint(\"Recall:\", recall)\nprecision = cnf_matrix[1][1]/(cnf_matrix[0][1]+cnf_matrix[1][1])\nprint(\"Precision:\", precision)\nprint(\"F1:\", 2*(recall*precision)/(recall+precision))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Recall: 0.814569536424\nPrecision: 0.872340425532\nF1: 0.842465753425\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["rfc_model.write().overwrite().save(\"/tmp/spark-random-forest-model\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"model","notebookId":3779541049641937},"nbformat":4,"nbformat_minor":0}
